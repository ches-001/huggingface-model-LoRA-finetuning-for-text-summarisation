{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Low Rank Adaptation and Parameter Efficient Finetuning of HuggingFace Alpaca LLMs on Text Summarisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, torch, json, random, gc\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    DataCollatorForSeq2Seq, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, Iterable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"LLM_REPOSITORY\"] = \"chavinlo/alpaca-native\"\n",
    "os.environ[\"TOKENIZER_REPOSITORY\"] = \"chavinlo/alpaca-native\"\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"DEVICE\"] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"DATASET_PATH\"] = \"data/doc_summary_data\"\n",
    "os.environ[\"TOKENS_DATA_PATH\"] = F\"data/doc_summary_{os.environ['TOKENIZER_REPOSITORY'].split('/')[-1]}_tokens\"\n",
    "os.environ[\"SUMMARY_DATA_PATH\"] = \"data/doc_summary_pair.json\"\n",
    "os.makedirs(os.environ[\"DATASET_PATH\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TOKENS_DATA_PATH\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset into Training, Validation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "with open(os.environ[\"SUMMARY_DATA_PATH\"]) as f:\n",
    "    doc_summary_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "train_size = int(len(doc_summary_data) * TRAIN_SIZE)\n",
    "val_size = int(len(doc_summary_data) * VALIDATION_SIZE)\n",
    "test_size = int(len(doc_summary_data) * TEST_SIZE)\n",
    "\n",
    "train_data = doc_summary_data[:train_size]\n",
    "val_data = doc_summary_data[train_size:train_size+val_size]\n",
    "test_data = doc_summary_data[train_size+val_size:]\n",
    "\n",
    "data_list = [\n",
    "    (\"train\", train_data),\n",
    "    (\"validation\", val_data),\n",
    "    (\"test\", test_data),\n",
    "]\n",
    "\n",
    "for data_tuple in data_list:\n",
    "    _file_path = os.path.join(os.environ[\"DATASET_PATH\"], f\"{data_tuple[0]}.json\")\n",
    "    if not os.path.exists(_file_path):\n",
    "        with open(_file_path, \"w\") as f:\n",
    "            json.dump(data_tuple[1], f, indent=4)\n",
    "        f.close()\n",
    "\n",
    "del doc_summary_data, train_data, val_data, test_data, data_list, data_tuple, train_size, val_size, test_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset into DictDataset Format to be modelled by the HuggingFace LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=os.environ[\"DATASET_PATH\"])\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corresponding LLM Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    os.environ[\"TOKENIZER_REPOSITORY\"],\n",
    "    model_max_length=int(os.environ[\"MAX_TOKENS\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tokenizer Object to retreive the Maximum Source (Text) and Target (Summary) Tokens in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataset = concatenate_datasets(\n",
    "    [dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]\n",
    ")\n",
    "tokenized_inputs = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"document\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "tokenized_targets = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]]) + 64\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]]) + 64\n",
    "\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset and Persist Tokens to Disk Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_PAD_TOKEN_ID = -100\n",
    "TRAIN_ON_INPUT = False\n",
    "generate_prompt = lambda document : (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\"\n",
    "    f\"Generate a concise summary of this document:\\n\\n DOCUMENT: \\n{document} \\n\\n SUMMARY:\"\n",
    ")\n",
    "def preprocess_function(\n",
    "    sample, \n",
    "    max_seq_length: int, \n",
    "    padding: str=\"max_length\", \n",
    "    train_on_input: bool=False):\n",
    "    \n",
    "    _input = f\"{generate_prompt(sample['document'])} {sample['summary']}\"\n",
    "    tokenization_result = tokenizer(\n",
    "        _input,\n",
    "        max_length=max_seq_length, \n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_tokens = tokenization_result[\"input_ids\"].copy()\n",
    "    label_tokens = tokenization_result[\"input_ids\"].copy()\n",
    "    \n",
    "    if not train_on_input:\n",
    "        prompt_tokens = tokenizer(\n",
    "            generate_prompt(sample['document']),\n",
    "            max_length=max_seq_length, \n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"]\n",
    "        prompt_tokens_len = len(prompt_tokens)\n",
    "        input_tokens = input_tokens[:prompt_tokens_len]\n",
    "        label_tokens = ([LABEL_PAD_TOKEN_ID]*prompt_tokens_len) + label_tokens[prompt_tokens_len:]\n",
    "        input_tokens = input_tokens + [tokenizer.pad_token_id]*(len(label_tokens)-prompt_tokens_len)\n",
    "        \n",
    "    else:\n",
    "        input_tokens.append(tokenizer.eos_token_id)\n",
    "        \n",
    "    tokenization_result[\"input_ids\"] = input_tokens\n",
    "    tokenization_result[\"labels\"] = label_tokens\n",
    "    tokenization_result[\"attention_mask\"] = [1]*len(input_tokens)\n",
    "    return tokenization_result\n",
    "\n",
    "preprocess_lambda = lambda dataset : preprocess_function(\n",
    "    dataset, \n",
    "    sum([max_source_length, max_target_length]),\n",
    "    train_on_input=TRAIN_ON_INPUT,\n",
    ")\n",
    "# batched arg must be set to False for this to work properly\n",
    "tokenized_dataset = dataset.map(preprocess_lambda, batched=False, remove_columns=[\"document\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"train\"))\n",
    "tokenized_dataset[\"validation\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"validation\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 8bits quantized HuggingFace LLM to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.environ[\"LLM_REPOSITORY\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Low Rank Adaptation Configurations Object and apply to Loaded LLM for Parameter Efficient Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q_proj\", \"v_proj\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Collator Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=LABEL_PAD_TOKEN_ID,\n",
    "    pad_to_multiple_of=8,\n",
    "     padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL FINETUNING / TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Seq2SeqTrainer Object and Commence LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = F\"lora-{os.environ['LLM_REPOSITORY'].split('/')[-1]}\"\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\tauto_find_batch_size=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # to be set to True for inference\n",
    "\n",
    "# finetune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist LoRA Model Weights to Disk Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "PEFT_MODEL_ID=f\"{os.environ['LLM_REPOSITORY'].split('/')[-1]}_finetuned_results\"\n",
    "trainer.model.save_pretrained(PEFT_MODEL_ID)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "# # delete model and tokenizer from memory\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LoRA Weights from Disk to Perform Inference on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = LlamaForCausalLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL_ID, device_map=\"auto\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Summaries from Documents in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_summaries = []\n",
    "n_docs = 5\n",
    "input_documents = dataset[\"test\"][\"document\"][:n_docs]\n",
    "target_summaries = dataset[\"test\"][\"summary\"][:n_docs]\n",
    "\n",
    "model.eval()\n",
    "for i, document in enumerate(input_documents):\n",
    "    prompt = generate_prompt(document)\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    summary_tokens = model.generate(\n",
    "        **prompt_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.15,\n",
    "        top_k=0,\n",
    "        repetition_penalty=1.1,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_tokens.sequences[0], skip_special_tokens=True)\n",
    "    print(f\"Document: {document}\\n\")\n",
    "    print(f\"SUMARY: {summary.replace(prompt, '').replace('</s>', '')}\\n\\n\")\n",
    "    predicted_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PERFORMANCE MEASUREMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Generated Summaries to Target Summaries with the Rouge Score and the Cosine Similarity Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "embeddings_model = SentenceTransformer(os.environ[\"EMBEDDINGS_MODEL\"])\n",
    "embeddings_model.to(os.environ[\"DEVICE\"])\n",
    "_zipped_data = zip(input_documents, predicted_summaries, target_summaries)\n",
    "\n",
    "for i, (document, predicted_summary, target_summary) in enumerate(_zipped_data):\n",
    "    prompt = generate_prompt(document)\n",
    "    pred_embeddings, target_embeddings = (\n",
    "        embeddings_model.encode(predicted_summary.replace(prompt, '').replace('</s>', '')).reshape(1, -1),\n",
    "        embeddings_model.encode(target_summary.replace(prompt, '').replace('</s>', '')).reshape(1, -1)\n",
    "    )\n",
    "    cos_similarity = cosine_similarity(target_embeddings, pred_embeddings)\n",
    "    rouge_scores = rouge.get_scores(predicted_summary, target_summary)\n",
    "    print(f\"Cosine similarity for summary {i+1}:\", cos_similarity[0][0], \"\\n\")\n",
    "    print(f\"Rouge scores for summary {i+1}:\", rouge_scores[0], \"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
