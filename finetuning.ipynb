{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, torch, json, random, gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"LLM_REPOSITORY\"] = \"google/flan-t5-small\"\n",
    "os.environ[\"TOKENIZER_REPOSITORY\"] = \"google/flan-t5-small\"\n",
    "os.environ[\"MAX_TOKENS\"] = \"4096\"\n",
    "os.environ[\"DEVICE\"] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"DATASET_PATH\"] = \"data/doc_summary_data\"\n",
    "os.environ[\"TOKENS_DATA_PATH\"] = \"data/doc_summary_tokens\"\n",
    "os.environ[\"SUMMARY_DATA_PATH\"] = \"data/doc_summary_pair.json\"\n",
    "os.makedirs(os.environ[\"DATASET_PATH\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TOKENS_DATA_PATH\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "with open(os.environ[\"SUMMARY_DATA_PATH\"]) as f:\n",
    "    doc_summary_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "train_size = int(len(doc_summary_data) * TRAIN_SIZE)\n",
    "val_size = int(len(doc_summary_data) * VALIDATION_SIZE)\n",
    "test_size = int(len(doc_summary_data) * TEST_SIZE)\n",
    "\n",
    "train_data = doc_summary_data[:train_size]\n",
    "val_data = doc_summary_data[train_size:train_size+val_size]\n",
    "test_data = doc_summary_data[train_size+val_size:]\n",
    "\n",
    "data_list = [\n",
    "    (\"train\", train_data),\n",
    "    (\"validation\", val_data),\n",
    "    (\"test\", test_data),\n",
    "]\n",
    "\n",
    "for data_tuple in data_list:\n",
    "    with open(os.path.join(os.environ[\"DATASET_PATH\"], f\"{data_tuple[0]}.json\"), \"w\") as f:\n",
    "        json.dump(data_tuple[1], f, indent=4)\n",
    "    f.close()\n",
    "\n",
    "del doc_summary_data, train_data, val_data, test_data, data_list, data_tuple, train_size, val_size, test_size\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/doc_summary_data to /home/ubuntu/.cache/huggingface/datasets/json/doc_summary_data-a190e415bec51eec/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d177cd549124fce9e31934cbb6b9c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0e1d896970418d8052ad1414929708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/doc_summary_data-a190e415bec51eec/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b1f7aecc2847fc85cf877b37faf831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1507\n",
      "Validation dataset size: 188\n",
      "Test dataset size: 189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 133,\n",
       " 'document': 'PURPOSE: This randomized phase III trial is studying chemotherapy and pelvic radiation therapy to see how well they work when given with or without additional chemotherapy in treating patients with high-risk early-stage cervical cancer after radical hysterectomy.\\nStudy Results: NO\\nConditions: Cervical Cancer\\nInterventions: DRUG: carboplatin|DRUG: cisplatin|DRUG: paclitaxel\\nPrimary Outcome Measures: Disease-free survival, From randomization to date of first failure (local, regional, or distant metastases failure or death due to any cause) or last follow-up. Analysis occurs after 43 disease-free survival failure events on Cisplatin/RT Arm.\\nSecondary Outcome Measures: Overall survival, From randomization to date of death or last follow-up. Analysis occurs after all patients have been potentially followed for 4 years.|Chemotherapy-induced neuropathy as measured by FACT-GOG/NTX4, From completion of concurrent chemoradiation to 12 months.|Quality of life as measured by FACT-Cx and FACIT-D, From completion of concurrent chemoradiation to 12 months.\\nOther Outcome Measures: unknown\\nSponsor: Radiation Therapy Oncology Group\\nCollaborators: National Cancer Institute (NCI)|NRG Oncology\\nSex: FEMALE\\nAge: ADULT, OLDER_ADULT\\nPhases: PHASE3\\nEnrollment: 238\\nFunder Type: NETWORK\\nStudy Type: INTERVENTIONAL\\nStudy Design: Allocation: RANDOMIZED|Intervention Model: PARALLEL|Masking: NONE|Primary Purpose: TREATMENT\\nOther IDs: RTOG-0724|CDR0000654709|NCI-2011-01973|RTOG 0724/GOG-0724\\nStart Date: 2009-09\\nPrimary Completion Date: 2023-08-31\\nCompletion Date: 2028-08-31\\nFirst Posted: 2009-09-21\\nResults First Posted: unknown\\nLast Update Posted: 2022-11-21',\n",
       " 'summary': 'This phase III trial is examining the effectiveness of different treatment approaches for high-risk early-stage cervical cancer after radical hysterectomy. The study is evaluating disease-free survival, overall survival, chemotherapy-induced neuropathy, and quality of life. It is sponsored by the Radiation Therapy Oncology Group and has enrolled 238 female adult and older adult participants. The trial began in September 2009 and is projected to conclude in August 2028.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(path=os.environ[\"DATASET_PATH\"])\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    os.environ[\"TOKENIZER_REPOSITORY\"],\n",
    "    model_max_length=int(os.environ[\"MAX_TOKENS\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 1363\n",
      "Max target length: 218\n"
     ]
    }
   ],
   "source": [
    "concatenated_dataset = concatenate_datasets(\n",
    "    [dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]\n",
    ")\n",
    "tokenized_inputs = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"document\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "tokenized_targets = concatenated_dataset.map(\n",
    "    lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"document\", \"summary\"])\n",
    "\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample, max_source_length: int, max_target_length: int, padding: str=\"max_length\"):\n",
    "    inputs = [f\"summarize this document: {item}\"  for item in sample[\"document\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_source_length, \n",
    "        padding=padding, \n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"summary\"], \n",
    "        max_length=max_target_length,\n",
    "        padding=padding, \n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else tokenizer.pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_lambda = lambda dataset : preprocess_function(dataset, max_source_length, max_target_length)\n",
    "tokenized_dataset = dataset.map(preprocess_lambda, batched=True, remove_columns=[\"document\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"train\"))\n",
    "tokenized_dataset[\"validation\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"validation\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(os.environ[\"TOKENS_DATA_PATH\"], \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.environ[\"LLM_REPOSITORY\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 77,649,280 || trainable%: 0.8862001038515747\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [945/945 10:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.213600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=945, training_loss=2.578261731667493, metrics={'train_runtime': 619.1577, 'train_samples_per_second': 12.17, 'train_steps_per_second': 1.526, 'total_flos': 3785013953740800.0, 'train_loss': 2.578261731667493, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = F\"lora-{os.environ['LLM_REPOSITORY'].split('/')[-1]}\"\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\tauto_find_batch_size=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # to be set to True for inference\n",
    "\n",
    "# finetune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finetuned_results/tokenizer_config.json',\n",
       " 'finetuned_results/special_tokens_map.json',\n",
       " 'finetuned_results/spiece.model',\n",
       " 'finetuned_results/added_tokens.json',\n",
       " 'finetuned_results/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "PEFT_MODEL_ID=\"finetuned_results\"\n",
    "trainer.model.save_pretrained(PEFT_MODEL_ID)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL_ID)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL_ID, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "# switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# define model pipeline\n",
    "hgf_pipeline = pipeline(\n",
    "    task=\"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.1, \n",
    "    max_length=int(os.environ[\"MAX_TOKENS\"]),\n",
    "    top_p=0.15,\n",
    "    top_k=0,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hgf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: page_content='NCT Number: NCT02289898\\nStudy Title: Study of Gemcitabine, Abraxane® Plus Placebo Versus Gemcitabine, Abraxane® Plus 1 or 2 Truncated Courses of Demcizumab in Subjects With 1st-Line Metastatic Pancreatic Ductal Adenocarcinoma\\nStudy URL: https://beta.clinicaltrials.gov/study/NCT02289898\\nAcronym: YOSEMITE\\nStudy Status: COMPLETED\\nBrief Summary: This is a randomized, double blind, 3 arm (1:1:1) study in subjects with 1st-line metastatic pancreatic ductal adenocarcinoma.' metadata={}\n",
      "\n",
      "SUMARY: This study aims to study the effectiveness of Demcizumab in treating 1st-line metastatic pancreatic ductal adenocarcinoma. The study is expected to be completed by June 2022.\n",
      "\n",
      "\n",
      "Document: page_content='The purpose is to test the efficacy and safety of demcizumab, when given in combination with gemcitabine and Abraxane® compared to placebo. The administration of gemcitabine and Abraxane® is a standard treatment for patients with metastatic pancreatic ductal adenocarcinoma.\\nStudy Results: YES\\nConditions: Pancreatic Cancer\\nInterventions: DRUG: Demcizumab|DRUG: Abraxane®|DRUG: gemcitabine|DRUG: Placebo\\nPrimary Outcome Measures: Hazard of Progression in the Placebo/Placebo Arm and the Pooled Demcizumab Arms, Investigator assessed Kaplan-Meier estimates of progression-free survival for placebo/placebo arm and pooled demcizumab arm., Investigator-assessed progression-free survival time through duration of the study (2 years, 23 days).\\nSecondary Outcome Measures: unknown\\nOther Outcome Measures: unknown\\nSponsor: OncoMed Pharmaceuticals, Inc.\\nCollaborators: Celgene Corporation\\nSex: ALL\\nAge: ADULT, OLDER_ADULT\\nPhases: PHASE2\\nEnrollment: 207\\nFunder Type: INDUSTRY\\nStudy Type: INTERVENTIONAL\\nStudy Design: Allocation: RANDOMIZED|Intervention Model: PARALLEL|Masking: TRIPLE (PARTICIPANT, INVESTIGATOR, OUTCOMES_ASSESSOR)|Primary Purpose: TREATMENT\\nOther IDs: M18-006\\nStart Date: 2015-04-20\\nPrimary Completion Date: 2017-05\\nCompletion Date: 2017-09\\nFirst Posted: 2014-11-13\\nResults First Posted: 2018-05-21\\nLast Update Posted: 2020-09-28' metadata={}\n",
      "\n",
      "SUMARY: The study aimed to test the effectiveness and safety of demcizumab in combination with gemcitabine and Abraxane®. The study aimed to assess the effectiveness and safety of demcizumab in combination with gemcitabine and Abraxane®. The study was sponsored by OncoMed Pharmaceuticals, Inc. and conducted by Celgene Corporation. The primary outcome measure was the risk of progression-free survival. The study was first posted in January 2018, and the last update was posted in September 2020.\n",
      "\n",
      "\n",
      "Document: page_content=\"Other IDs: M18-006\\nStart Date: 2015-04-20\\nPrimary Completion Date: 2017-05\\nCompletion Date: 2017-09\\nFirst Posted: 2014-11-13\\nResults First Posted: 2018-05-21\\nLast Update Posted: 2020-09-28\\nLocations: Banner MD Anderson Cancer Center, Gilbert, Arizona, 85234, United States|Providence Saint Joseph Medical Center, Burbank, California, 91505, United States|City of Hope, Duarte, California, 91010, United States|Scripps Cancer Center, La Jolla, California, 92037, United States|University of California, Davis Comprehensive Cancer Center, Sacramento, California, 95817, United States|Soulhern California Permanente Medical Group, San Marcos, California, 92078, United States|Kaiser Permanente Medical Center, Vallejo, California, 94589, United States|Rocky Mountain Cancer Centers, Denver, Colorado, 80218, United States|Lynn Cancer Institute, Boca Raton, Florida, 33486, United States|University of Iowa Hospitals and Clinics, Iowa City, Iowa, 52242, United States|University of Kansas Cancer Center, Westwood, Kansas, 66205, United States|Ochsner Clinic Foundation, New Orleans, Louisiana, 70121, United States|University of Michigan, Ann Arbor, Michigan, 48109, United States|Dartmouth Hitchcock Medical Center, Norris Cotton Cancer Center, Lebanon, New Hampshire, 03756, United States|University of Rochester, Rochester, New York, 14642, United States|SUNY Upstate Medical University, Syracuse, New York, 13210, United States|Cleveland Clinic, Cleveland, Ohio, 44195, United States|Kaiser Permanente NW Oncology Research, Portland, Oregon, 97227, United States|Thomas Jefferson University, Sydney Kimmel Cancer Center, Philadelphia, Pennsylvania, 19107, United States|Baylor College of Medicine, Houston, Texas, 77030, United States|Joe Arrington Cancer Research Treatment Center, Lubbock, Texas, 79410, United States|Huntsman Cancer Institute at The University of Utah, Salt Lake City, Utah, 84112, United States|Prince of Wales Hospital, Randwick, New South Wales, 2031, Australia|Monash Medical Centre, Moorabbin, Bentleigh East, Victoria, 3165, Australia|Western Health (Sunshine Hospitals), St Albans, Victoria, 3021, Australia|St John of God Murdoch Hospital, Murdoch, Western Australia, 6150, Australia|St. John of God Subiaco Hospital, Subiaco, Western Australia, 6008, Australia|Hopital Erasme, Brussels, Brussels Capital, 1070, Belgium|Tom Baker Cancer Centre, Calgary, Alberta, T2N 4N2, Canada|British Columbia Cancer Agency, Vancouver, British Columbia, V5Z 4E6, Canada|QEII Health Sciences Centre, Halifax, Nova Scotia, B3H 2Y9, Canada|London Regional Cancer Program, London, Ontario, N6A 4L6, Canada|Sunnybrook Health Sciences Centre, Odette Cancer Centre, Toronto, Ontario, M4N 3M5, Canada|Princess Margaret Hospital, Toronto, Ontario, M5G 2M9, Canada|St Josephs Health Centre, Toronto, Ontario, M6R 1B5, Canada|Hospital Universitario de Fuenlabrada, Fuenlabrada, Madrid, 28492, Spain|Hospital Universitari Germans Trias i Pujol - lnstituto Catalan d'Oncologia (!CO), Barcelona, 08916, Spain|Hospital General Universitario Gregorio Marafi6n, Madrid, 28007, Spain|Hospital Universitario de Fuenlabrada, Madrid, 28942, Spain|Hospital Universitario Miguel Servet, Zaragoza, 50009, Spain|Bristol Haematology & Oncology Centre, Bristol, BS2 8ED, United Kingdom|Sarah Cannon Research Institute UK, London, W1G 6AD, United Kingdom\\nStudy Documents: Statistical Analysis Plan|Study Protocol and Informed Consent Form\" metadata={}\n",
      "\n",
      "SUMARY: The study is being conducted at Banner MD Anderson Cancer Center in Gilbert, Arizona, United States. The results were posted in May 2018, and the study documents included the statistical analysis plan and informed consent form.\n",
      "\n",
      "\n",
      "Document: page_content='NCT Number: NCT03578081\\nStudy Title: Olanzapine With or Without Fosaprepitant Dimeglumine in Preventing Chemotherapy Induced Nausea and Vomiting in Cancer Patients Receiving Highly Emetogenic Chemotherapy\\nStudy URL: https://beta.clinicaltrials.gov/study/NCT03578081\\nAcronym: unknown\\nStudy Status: COMPLETED\\nBrief Summary: This randomized phase III trial studies how well olanzapine with or without fosaprepitant work in preventing chemotherapy induced nausea and vomiting in cancer patients receiving chemotherapy that causes vomiting. Olanzapine and fosaprepitant dimeglumine may help control nausea and vomiting in patients during chemotherapy. Olanzapine is usually given in combination with other drugs, including fosaprepitant dimeglumine. It is not yet known if olanzapine when given with other drugs, is still effective without using fosaprepitant dimeglumine for controlling nausea and vomiting.\\nStudy Results: YES\\nConditions: Malignant Neoplasm\\nInterventions: DRUG: Palonosetron Hydrochloride|DRUG: Ondansetron Hydrochloride|DRUG: Dexamethasone|DRUG: Fosaprepitant Dimeglumine|DRUG: Olanzapine|OTHER: Placebo\\nPrimary Outcome Measures: No Nausea Rate Defined as a Response of 0 in the Nausea Item of Nausea and Vomiting Daily Diary/Questionnaire in the Overall (0-120 Hours), Acute (0-24 Hours), and Delayed (24-120 Hours) Periods, The specific measure will be based on the proportion of patients with a value of 0, as measured by the single nausea item (scale 0-10, 0 is no symptoms, 10 is worst symptoms) of the Questionnaire. A modified intent-to-treat principle will be applied for statistical analysis of efficacy in evaluable patients. The proportions of patients with no nausea during the overall, the acute, and the delayed period will be summarized by treatment arm. They will be tested in a sequential manner, using a Simes gatekeeping procedure to maintain the overall significance level at the specified by the Lan-DeMets family of alpha spending function. The difference in no nausea proportions between arms will be estimated along with a one-sided 95% confidence interval. The tests and the confidence intervals will be constructed using normal approximation of the binomial distribution adjusted for the non-inferiority margin., Up to 120 hours' metadata={}\n",
      "\n",
      "SUMARY: This study aims to evaluate the effectiveness of olanzapine with or without fosaprepitant dimeglumine in preventing induced nausea and vomiting in cancer patients receiving highly emetogenic chemotherapy. The study is currently active and has been completed.\n",
      "\n",
      "\n",
      "Document: page_content='Secondary Outcome Measures: Complete Response (CR) (no Emetic Episodes and no Use of Rescue Medication) During the Acute, Delayed and the Overall Periods as Measured by the Nausea and Vomiting Daily Diary/Questionnaire, The specific measure will be based on the proportion of patients who answered \"None\" to both questions concerning Vomiting episode(None, Once, Twice, More than twice) and number of extra nausea/vomiting pills taken (None, One, Two, More than two) in the Nausea and Vomiting Daily Diary/Questionnaire. The CR rate for the overall, the acute, and the delayed period will be summarized by treatment arm and will be compared using a Chi-squared test. The difference in CR rates between arms will be estimated along with a 95% confidence interval., Up to 120 hours|Potential Toxicities as Ascribed to Olanzapine as Measured by the Nausea and Vomiting Daily Diary/Questionnaire, Potential toxicities includes nausea, undesired sedation, and undesired appetite, measured by three individual items ( nausea, undesired sedation, undesired appetite) of the Nausea and Vomiting Daily Dairy/Questionnaire(scale 0-10, 0 is no symptoms, 10 is worst symptoms). Incidences of toxicities will be summarized by type and by treatment arm. Incidences of toxicities will be compared between arms using a Chi-squared test or the Fisher\\'s exact test as appropriate. In addition, undesired sedation and appetite increase as collected in the Nausea and Vomiting Daily Diary/Questionnaire will be analyzed by repeated measures analyses including descriptive statistics, graphical approaches, and growth curve models to account for the factor of day and time trend., Up to 120 hours|Average Nausea Scores (0-10) Repeatedly Measured by the Nausea and Vomiting Daily Diary/Questionnaire, The specific measure will be based on the by the single nausea item (scale 0-10, 0 is no symptoms, 10 is worst symptoms) of the Nausea and Vomiting Daily Diary/Questionnaire. Nausea scores (0-10) repeatedly measured by the Nausea and Vomiting Daily Diary/Questionnaire will be analyzed using the repeated measures analyses and growth curve models as described above for undesired sedation and increase in appetite., Up to 1 year|Total Frequency of Rescue Medication as Measured by the Nausea and Vomiting Daily Diary/Questionnaire, The specific measure will be based on the single item : number of extra nausea/vomiting pills taken (None, One , Twice, More than twice) of the Nausea and Vomiting Daily Diary/Questionnaire., Over 5 Days per each of the 4 cycles\\nOther Outcome Measures: unknown\\nSponsor: Alliance for Clinical Trials in Oncology\\nCollaborators: National Cancer Institute (NCI)\\nSex: ALL\\nAge: ADULT, OLDER_ADULT\\nPhases: PHASE3\\nEnrollment: 690\\nFunder Type: OTHER\\nStudy Type: INTERVENTIONAL\\nStudy Design: Allocation: RANDOMIZED|Intervention Model: PARALLEL|Masking: DOUBLE (PARTICIPANT, INVESTIGATOR)|Primary Purpose: SUPPORTIVE_CARE\\nOther IDs: A221602|NCI-2017-02410|UG1CA189823\\nStart Date: 2018-10-15\\nPrimary Completion Date: 2021-11-08\\nCompletion Date: 2023-05-01\\nFirst Posted: 2018-07-05\\nResults First Posted: 2023-02-09\\nLast Update Posted: 2023-05-08' metadata={}\n",
      "\n",
      "SUMARY: The Alliance for Clinical Trials in Oncology conducted a study in the United States and conducted a study in collaboration with the National Cancer Institute. The study was conducted by Alliance for Clinical Trials in Oncology and was sponsored by Alliance for Clinical Trials in Oncology. The primary outcome measure was complete response, and the primary outcome measure was the total frequency of rescue medication. The study was completed on October 15, 2023.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "# summarise first 5 documens in the testing data\n",
    "n_ducs = 5\n",
    "for i, document in enumerate(dataset[\"test\"][\"document\"][:n_ducs]):\n",
    "    document = Document(page_content=document)\n",
    "    print(f\"Document: {document}\\n\")\n",
    "    print(f\"SUMARY: {summary_chain.run([document])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
