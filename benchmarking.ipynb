{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578fe30e-1779-4523-a57f-226b0c86dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, torch, json, random\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    LlamaTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoConfig,\n",
    "    LlamaConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from peft import PeftConfig\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, Iterable, Union, Any\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"EMBEDDINGS_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "os.environ[\"MAX_TOKENS\"] = \"2048\"\n",
    "os.environ[\"DEVICE\"] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"DATASET_PATH\"] = \"data/doc_summary_data\"\n",
    "os.environ[\"FINETUNED_FLAN_T5_ID\"] = \"flan-t5-small_finetuned_results\"\n",
    "os.environ[\"ALPACA_LLM\"] = \"chavinlo/alpaca-native\"\n",
    "os.environ[\"VICUNA_LLM\"] = \"lmsys/vicuna-13b-v1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b2f672-401d-40d8-904a-0e5333b923e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_test_data(\n",
    "    model: Union[AutoModelForSeq2SeqLM, AutoModelForCausalLM, LlamaForCausalLM], \n",
    "    tokenizer: Union[AutoTokenizer, LlamaTokenizer], \n",
    "    dataset_key: str=\"test\",\n",
    "    task=\"text2text-generation\",\n",
    "    n_docs: int = 5,\n",
    "    log_summary: bool=False,\n",
    "    log_metrics: bool=False,\n",
    "    delete_llm_after_use: bool=False,\n",
    "    **kwargs):\n",
    "    \n",
    "    # switch model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Define model pipeline for inference with langchain\n",
    "    kwargs = {**dict(temperature=0.1, top_p=0.15, top_k=0, repetition_penalty=1.1), **kwargs}\n",
    "\n",
    "    # define model pipeline\n",
    "    hgf_pipeline = pipeline(\n",
    "        task=task, \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        max_length=int(os.environ[\"MAX_TOKENS\"]),\n",
    "        **kwargs\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=hgf_pipeline)\n",
    "    \n",
    "    # Define Summary chain\n",
    "    summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(path=os.environ[\"DATASET_PATH\"])\n",
    "    \n",
    "    # Generate Summaries and Measure Performance (Rouge Metric and Cosine Similarity Metric)\n",
    "    rouge = Rouge()  # rouge metric object\n",
    "    embeddings_model = SentenceTransformer(os.environ[\"EMBEDDINGS_MODEL\"]) # embeddings model object\n",
    "    embeddings_model.to(os.environ[\"DEVICE\"])\n",
    "    documents = dataset[dataset_key][\"document\"][:n_docs]\n",
    "    target_summaries = dataset[dataset_key][\"summary\"][:n_docs]\n",
    "    _zipped = zip(documents, target_summaries)\n",
    "    metrics_values: Iterable[Dict[str, Any]] = []\n",
    "\n",
    "    for i, (document, target_summary) in enumerate(_zipped):\n",
    "        document = Document(page_content=document)\n",
    "        try:\n",
    "            generated_summary = summary_chain.run([document])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error summarizing document-{i+1}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        if log_summary:\n",
    "            print(f\"GENERATED SUMARY: {generated_summary}\\n\")\n",
    "            print(f\"TARGET SUMARY: {target_summary}\\n\")\n",
    "\n",
    "        generated_summary_embeddings, target_summary_embeddings = (\n",
    "            embeddings_model.encode(generated_summary).reshape(1, -1),\n",
    "            embeddings_model.encode(target_summary).reshape(1, -1)\n",
    "        )\n",
    "        cos_similarity = cosine_similarity(target_summary_embeddings, generated_summary_embeddings)\n",
    "        rouge_scores = rouge.get_scores(generated_summary, target_summary)\n",
    "        if log_metrics:\n",
    "            print(f\"Cosine similarity for summary {i+1}:\", cos_similarity[0][0], \"\\n\")\n",
    "            print(f\"Rouge scores for summary {i+1}:\", rouge_scores[0], \"\\n\")\n",
    "            \n",
    "        if log_metrics or log_summary:\n",
    "            print(\"\\n\")\n",
    "        _metric = dict(semantic_similarity=cos_similarity, rouge_scores=rouge_scores)\n",
    "        metrics_values.append(_metric)\n",
    "        \n",
    "    if delete_llm_after_use:\n",
    "        #model.to(torch.device(\"cpu\"))\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return metrics_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885c01cb-8452-4b74-82a1-e32bc2307fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_DATASET_KEY = \"test\"\n",
    "N_INFERENCE_DOCS = 5\n",
    "LOG_SUMMARY = True\n",
    "LOG_METRICS = True\n",
    "DELETE_LLM_AFTER_USE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b42b7c-c5f3-41e0-b00d-c5a65c90a773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/doc_summary_data-786ffbbe80ba07a0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefafc0e5e1849b9a79f9f848d494bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SUMARY: Study of Gemcitabine, Abraxane® Plus Placebo Versus Gemcitabine, Abraxane® Plus 1 or 2 Truncated Courses of Demcizumab in Subjects With 1st-Line Metastatic Pancreatic Ductal Adenocarcinoma Study\n",
      "\n",
      "TARGET SUMARY: The completed study, known as YOSEMITE, investigated the treatment of 1st-line metastatic pancreatic ductal adenocarcinoma. It was a randomized, double-blind study with three arms. The study has the NCT Number NCT02289898.\n",
      "\n",
      "Cosine similarity for summary 1: 0.57218146 \n",
      "\n",
      "Rouge scores for summary 1: {'rouge-1': {'r': 0.03571428571428571, 'p': 0.047619047619047616, 'f': 0.04081632163265365}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03571428571428571, 'p': 0.047619047619047616, 'f': 0.04081632163265365}} \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SUMARY: Demcizumab is a standard treatment for patients with metastatic pancreatic ductal adenocarcinoma.\n",
      "\n",
      "TARGET SUMARY: This study evaluated the effectiveness and safety of demcizumab in combination with gemcitabine and Abraxane® for treating metastatic pancreatic ductal adenocarcinoma. The study, sponsored by OncoMed Pharmaceuticals, Inc., included 207 participants. The primary outcome measures were the progression-free survival rates in the placebo and demcizumab arms. The results were initially published in May 2018.\n",
      "\n",
      "Cosine similarity for summary 2: 0.86372316 \n",
      "\n",
      "Rouge scores for summary 2: {'rouge-1': {'r': 0.13333333333333333, 'p': 0.5, 'f': 0.210526312465374}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.2727272727272727, 'f': 0.09230768949585806}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.4166666666666667, 'f': 0.17543859316712837}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: The United States has a population of 74,769, and a population of 87,769, in the United States.\n",
      "\n",
      "TARGET SUMARY: The study with ID M18-006 began in April 2015 and ended in September 2017. The results were posted in May 2018 and last updated in September 2020. It took place in multiple locations across several countries and includes documents such as a statistical analysis plan, study protocol, and informed consent form.\n",
      "\n",
      "Cosine similarity for summary 3: 0.0780949 \n",
      "\n",
      "Rouge scores for summary 3: {'rouge-1': {'r': 0.0975609756097561, 'p': 0.3333333333333333, 'f': 0.15094339272338916}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.3333333333333333, 'f': 0.15094339272338916}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: The study focuses on the proportion of patients with a value of 0 in the Nausea Item of Nausea and Vomiting Daily Diary/Questionnaire in the Overall (0-120 Hours), Acute (0-24 Hours), and Delayed (24-120 Hours) Periods.\n",
      "\n",
      "TARGET SUMARY: This study examined the effectiveness of olanzapine with or without fosaprepitant in preventing chemotherapy-induced nausea and vomiting in cancer patients. The results showed that olanzapine alone was effective in controlling these symptoms. The study measured the proportion of patients with no nausea at different time periods and used a sequential testing procedure to analyze the data.\n",
      "\n",
      "Cosine similarity for summary 4: 0.53085 \n",
      "\n",
      "Rouge scores for summary 4: {'rouge-1': {'r': 0.22727272727272727, 'p': 0.35714285714285715, 'f': 0.2777777730246914}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.14705882352941177, 'f': 0.11235954584017191}, 'rouge-l': {'r': 0.22727272727272727, 'p': 0.35714285714285715, 'f': 0.2777777730246914}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: The rate for the overall, acute, and delayed period will be estimated along with a 95% confidence interval.\n",
      "\n",
      "TARGET SUMARY: This study evaluates the effectiveness and side effects of olanzapine in preventing nausea and vomiting in cancer patients undergoing chemotherapy. It measures the proportion of patients without vomiting episodes and assesses potential side effects. The study is a phase 3 randomized trial with 690 participants and is expected to be completed in 2023. The results will be posted in February 2023 and the study is sponsored by the Alliance for Clinical Trials in Oncology in collaboration with the National Cancer Institute.\n",
      "\n",
      "Cosine similarity for summary 5: 0.09367725 \n",
      "\n",
      "Rouge scores for summary 5: {'rouge-1': {'r': 0.14545454545454545, 'p': 0.4444444444444444, 'f': 0.219178078476262}, 'rouge-2': {'r': 0.01282051282051282, 'p': 0.058823529411764705, 'f': 0.021052628640443626}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.3333333333333333, 'f': 0.16438355792831685}} \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load peft config for pre-trained checkpoint etc. \n",
    "config = PeftConfig.from_pretrained(os.environ[\"FINETUNED_FLAN_T5_ID\"])\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "flan_t5_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.base_model_name_or_path,  \n",
    "    load_in_8bit=True,  \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "flan_t5_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    max_length=os.environ[\"MAX_TOKENS\"],\n",
    ")\n",
    "\n",
    "flan_t5_performance = run_on_test_data(\n",
    "    flan_t5_model, \n",
    "    flan_t5_tokenizer, \n",
    "    INFERENCE_DATASET_KEY, \n",
    "    task=\"text2text-generation\", \n",
    "    n_docs=N_INFERENCE_DOCS,\n",
    "    log_summary=LOG_SUMMARY,\n",
    "    log_metrics=LOG_METRICS,\n",
    "    delete_llm_after_use=DELETE_LLM_AFTER_USE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f24e94b-56da-46de-8295-45df2d86ca4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7f6f60468340a396fae8cef20bb707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/doc_summary_data-786ffbbe80ba07a0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f7b74ef7584089b51517cdaadfaedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SUMARY: \n",
      "The YOSEMITE clinical trial involved three arms for patients with first-line metastatic pancreatic ductal adenocarcinoma. The study compared gemcitabine, Abraxane® plus placebo to gemcitabine, Abraxane® plus one or two truncated courses of demcizumab.\n",
      "\n",
      "TARGET SUMARY: The completed study, known as YOSEMITE, investigated the treatment of 1st-line metastatic pancreatic ductal adenocarcinoma. It was a randomized, double-blind study with three arms. The study has the NCT Number NCT02289898.\n",
      "\n",
      "Cosine similarity for summary 1: 0.768512 \n",
      "\n",
      "Rouge scores for summary 1: {'rouge-1': {'r': 0.35714285714285715, 'p': 0.3448275862068966, 'f': 0.35087718798399514}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.16666666666666666, 'f': 0.16666666166666683}, 'rouge-l': {'r': 0.32142857142857145, 'p': 0.3103448275862069, 'f': 0.31578946868574953}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: \n",
      "A clinical trial evaluated the effectiveness and safety of demcizumab, a drug used in combination with gemcitabine and Abraxane®, for treating metastatic pancreatic ductal adenocarcinoma. Results showed that the combination therapy increased progression-free survival time compared to standard treatment alone. The study was sponsored by OncoMed Pharmaceuticals and conducted in collaboration with Celgene Corporation. It involved 207 participants and took place at multiple sites in the US.\n",
      "\n",
      "TARGET SUMARY: This study evaluated the effectiveness and safety of demcizumab in combination with gemcitabine and Abraxane® for treating metastatic pancreatic ductal adenocarcinoma. The study, sponsored by OncoMed Pharmaceuticals, Inc., included 207 participants. The primary outcome measures were the progression-free survival rates in the placebo and demcizumab arms. The results were initially published in May 2018.\n",
      "\n",
      "Cosine similarity for summary 2: 0.9787829 \n",
      "\n",
      "Rouge scores for summary 2: {'rouge-1': {'r': 0.5555555555555556, 'p': 0.43103448275862066, 'f': 0.4854368882835329}, 'rouge-2': {'r': 0.35185185185185186, 'p': 0.2878787878787879, 'f': 0.31666666171666674}, 'rouge-l': {'r': 0.5555555555555556, 'p': 0.43103448275862066, 'f': 0.4854368882835329}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: \n",
      "The M18-006 clinical trial is evaluating the effectiveness of a specific treatment regimen for patients with advanced colorectal cancer. It is taking place at multiple locations in the US and other countries including Australia, Belgium, Canada, and Spain. The original completion date was May 2017, but it has been extended to September 2017. Results have been posted as early as May 2018, with updates as recent as September 2020.\n",
      "\n",
      "TARGET SUMARY: The study with ID M18-006 began in April 2015 and ended in September 2017. The results were posted in May 2018 and last updated in September 2020. It took place in multiple locations across several countries and includes documents such as a statistical analysis plan, study protocol, and informed consent form.\n",
      "\n",
      "Cosine similarity for summary 3: 0.60428137 \n",
      "\n",
      "Rouge scores for summary 3: {'rouge-1': {'r': 0.4146341463414634, 'p': 0.29310344827586204, 'f': 0.34343433858177735}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.04411764705882353, 'f': 0.051282046413909446}, 'rouge-l': {'r': 0.3902439024390244, 'p': 0.27586206896551724, 'f': 0.32323231837975724}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: \n",
      "In this study, researchers compared the effectiveness of olanzapine alone or combined with fosaprepitant dimeglumine in preventing chemotherapy-induced nausea and vomiting (CINV) in cancer patients receiving highly emetogenic chemotherapy. Both treatments proved to be effective in reducing nausea and vomiting, but there was no significant difference between them.\n",
      "\n",
      "TARGET SUMARY: This study examined the effectiveness of olanzapine with or without fosaprepitant in preventing chemotherapy-induced nausea and vomiting in cancer patients. The results showed that olanzapine alone was effective in controlling these symptoms. The study measured the proportion of patients with no nausea at different time periods and used a sequential testing procedure to analyze the data.\n",
      "\n",
      "Cosine similarity for summary 4: 0.9410372 \n",
      "\n",
      "Rouge scores for summary 4: {'rouge-1': {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}, 'rouge-2': {'r': 0.21818181818181817, 'p': 0.2608695652173913, 'f': 0.23762375741593972}, 'rouge-l': {'r': 0.38636363636363635, 'p': 0.38636363636363635, 'f': 0.3863636313636364}} \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SUMARY: \n",
      "A recent study compared the effectiveness of olanzapine to standard antiemetic therapy in preventing chemotherapy-induced nausea and vomiting (CINV) in patients receiving highly emetogenic chemotherapy. The study enrolled 690 participants at six US clinical sites and found that olanzapine was more effective than standard therapy in achieving complete response and reducing nausea scores. However, it also had a higher rate of drowsiness as a side effect. The study's primary completion date was November 8, 2021, and its results were first posted on February 9, 2023, with final updates posted on May 8, 2023.\n",
      "\n",
      "TARGET SUMARY: This study evaluates the effectiveness and side effects of olanzapine in preventing nausea and vomiting in cancer patients undergoing chemotherapy. It measures the proportion of patients without vomiting episodes and assesses potential side effects. The study is a phase 3 randomized trial with 690 participants and is expected to be completed in 2023. The results will be posted in February 2023 and the study is sponsored by the Alliance for Clinical Trials in Oncology in collaboration with the National Cancer Institute.\n",
      "\n",
      "Cosine similarity for summary 5: 0.92486763 \n",
      "\n",
      "Rouge scores for summary 5: {'rouge-1': {'r': 0.41818181818181815, 'p': 0.3026315789473684, 'f': 0.351145033296428}, 'rouge-2': {'r': 0.08974358974358974, 'p': 0.07777777777777778, 'f': 0.08333332835884383}, 'rouge-l': {'r': 0.41818181818181815, 'p': 0.3026315789473684, 'f': 0.351145033296428}} \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(os.environ[\"ALPACA_LLM\"])\n",
    "\n",
    "vicuna_model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.environ[\"VICUNA_LLM\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "vicuna_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    os.environ[\"VICUNA_LLM\"], \n",
    "    max_length=config.max_sequence_length,\n",
    ")\n",
    "\n",
    "vicuna_performance = run_on_test_data(\n",
    "    vicuna_model, \n",
    "    vicuna_tokenizer, \n",
    "    INFERENCE_DATASET_KEY, \n",
    "    task=\"text-generation\", \n",
    "    n_docs=N_INFERENCE_DOCS,\n",
    "    log_summary=LOG_SUMMARY,\n",
    "    log_metrics=LOG_METRICS,\n",
    "    delete_llm_after_use=DELETE_LLM_AFTER_USE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7df041-2cdb-46f8-81c6-ea8d2601e735",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8bcb6009a61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ALPACA_LLM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m alpaca_model = LlamaForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ALPACA_LLM\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2840\u001b[0m                 }\n\u001b[1;32m   2841\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   2843\u001b[0m                         \"\"\"\n\u001b[1;32m   2844\u001b[0m                         \u001b[0mSome\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdispatched\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMake\u001b[0m \u001b[0msure\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0menough\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mRAM\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(os.environ[\"ALPACA_LLM\"])\n",
    "\n",
    "alpaca_model = LlamaForCausalLM.from_pretrained(\n",
    "    os.environ[\"ALPACA_LLM\"],\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "alpaca_tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    os.environ[\"ALPACA_LLM\"], \n",
    "    max_length=config.max_sequence_length,\n",
    ")\n",
    "\n",
    "alpaca_performance = run_on_test_data(\n",
    "    vicuna_model, \n",
    "    vicuna_tokenizer, \n",
    "    INFERENCE_DATASET_KEY, \n",
    "    task=\"text-generation\", \n",
    "    n_docs=N_INFERENCE_DOCS,\n",
    "    log_summary=LOG_SUMMARY,\n",
    "    log_metrics=LOG_METRICS,\n",
    "    delete_llm_after_use=DELETE_LLM_AFTER_USE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
